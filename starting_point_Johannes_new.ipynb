{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read me"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to start with pre-filtered data set, start with import section and then skip everything until \"3. Starting point\". Start from there"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_hub\n",
      "  Downloading tensorflow_hub-0.13.0-py2.py3-none-any.whl (100 kB)\n",
      "     -------------------------------------- 100.6/100.6 kB 2.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: protobuf>=3.19.6 in c:\\users\\johan\\anaconda3\\lib\\site-packages (from tensorflow_hub) (4.22.1)\n",
      "Requirement already satisfied: numpy>=1.12.0 in c:\\users\\johan\\anaconda3\\lib\\site-packages (from tensorflow_hub) (1.23.5)\n",
      "Installing collected packages: tensorflow_hub\n",
      "Successfully installed tensorflow_hub-0.13.0\n"
     ]
    }
   ],
   "source": [
    "#statements for ucloud\n",
    "#!pip install pandas\n",
    "#!pip install numpy\n",
    "#!pip install seaborn\n",
    "#!pip install nltk\n",
    "#!pip install matplotlib\n",
    "#!pip install scikit-learn\n",
    "#!pip install tensorflow\n",
    "#!pip install tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\johan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\johan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\johan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import seaborn as sns \n",
    "\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, WordNetLemmatizer\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Starting point"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Reading in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list with the names of the files\n",
    "file_names = ['newsdata_1.csv', 'newsdata_2.csv', 'newsdata_3.csv', 'newsdata_4.csv', 'newsdata_5.csv', 'newsdata_6.csv', 'newsdata_7.csv', 'newsdata_8.csv', 'newsdata_9.csv', 'newsdata_10.csv', 'newsdata_11.csv', 'newsdata_12.csv', 'newsdata_13.csv', 'newsdata_14.csv', 'newsdata_15.csv', 'newsdata_16.csv', 'newsdata_17.csv', 'newsdata_18.csv', 'newsdata_19.csv', 'newsdata_20.csv', 'newsdata_21.csv', 'newsdata_22.csv', 'newsdata_23.csv', 'newsdata_24.csv', 'newsdata_25.csv', 'newsdata_26.csv', 'newsdata_27.csv', 'newsdata_28.csv', 'newsdata_29.csv', 'newsdata_30.csv', 'newsdata_31.csv', 'newsdata_32.csv', 'newsdata_33.csv', 'newsdata_34.csv', 'newsdata_35.csv', 'newsdata_36.csv', 'newsdata_37.csv', 'newsdata_38.csv', 'newsdata_39.csv', 'newsdata_40.csv', 'newsdata_41.csv', 'newsdata_42.csv', 'newsdata_43.csv', 'newsdata_44.csv', 'newsdata_45.csv', 'newsdata_46.csv', 'newsdata_47.csv', 'newsdata_48.csv', 'newsdata_49.csv', 'newsdata_50.csv', 'newsdata_51.csv', 'newsdata_52.csv', 'newsdata_53.csv', 'newsdata_54.csv', 'newsdata_55.csv', 'newsdata_56.csv', 'newsdata_57.csv', 'newsdata_58.csv', 'newsdata_59.csv', 'newsdata_60.csv', 'newsdata_61.csv', 'newsdata_62.csv', 'newsdata_63.csv', 'newsdata_64.csv', 'newsdata_65.csv', 'newsdata_66.csv', 'newsdata_67.csv', 'newsdata_68.csv', 'newsdata_69.csv', 'newsdata_70.csv', 'newsdata_71.csv',\n",
    "'newsdata_72.csv', 'newsdata_73.csv', 'newsdata_74.csv', 'newsdata_75.csv', 'newsdata_76.csv', 'newsdata_77.csv', 'newsdata_78.csv', 'newsdata_79.csv', 'newsdata_80.csv', 'newsdata_81.csv', 'newsdata_82.csv', 'newsdata_83.csv', 'newsdata_84.csv', 'newsdata_85.csv', 'newsdata_86.csv', 'newsdata_87.csv', 'newsdata_88.csv', 'newsdata_89.csv', 'newsdata_90.csv', 'newsdata_91.csv', 'newsdata_92.csv', 'newsdata_93.csv', 'newsdata_94.csv', 'newsdata_95.csv', 'newsdata_96.csv', 'newsdata_97.csv', 'newsdata_98.csv', 'newsdata_99.csv', 'newsdata_100.csv', 'newsdata_101.csv', 'newsdata_102.csv', 'newsdata_103.csv', 'newsdata_104.csv', 'newsdata_105.csv', 'newsdata_106.csv', 'newsdata_107.csv', 'newsdata_108.csv', 'newsdata_109.csv', 'newsdata_110.csv', 'newsdata_111.csv', 'newsdata_112.csv', 'newsdata_113.csv', 'newsdata_114.csv', 'newsdata_115.csv', 'newsdata_116.csv', 'newsdata_117.csv', 'newsdata_118.csv', 'newsdata_119.csv', 'newsdata_120.csv', 'newsdata_121.csv', 'newsdata_122.csv', 'newsdata_123.csv', 'newsdata_124.csv', 'newsdata_125.csv', 'newsdata_126.csv', 'newsdata_127.csv', 'newsdata_128.csv', 'newsdata_129.csv', 'newsdata_130.csv', 'newsdata_131.csv', 'newsdata_132.csv', 'newsdata_133.csv', 'newsdata_134.csv', 'newsdata_135.csv', 'newsdata_136.csv', 'newsdata_137.csv', 'newsdata_138.csv', 'newsdata_139.csv', 'newsdata_140.csv', 'newsdata_141.csv', 'newsdata_142.csv',\n",
    "'newsdata_143.csv', 'newsdata_144.csv', 'newsdata_145.csv', 'newsdata_146.csv', 'newsdata_147.csv', 'newsdata_148.csv', 'newsdata_149.csv', 'newsdata_150.csv', 'newsdata_151.csv', 'newsdata_152.csv', 'newsdata_153.csv', 'newsdata_154.csv', 'newsdata_155.csv', 'newsdata_156.csv', 'newsdata_157.csv', 'newsdata_158.csv', 'newsdata_159.csv', 'newsdata_160.csv', 'newsdata_161.csv', 'newsdata_162.csv', 'newsdata_163.csv', 'newsdata_164.csv', 'newsdata_165.csv', 'newsdata_166.csv', 'newsdata_167.csv', 'newsdata_168.csv', 'newsdata_169.csv', 'newsdata_170.csv', 'newsdata_171.csv', 'newsdata_172.csv', 'newsdata_173.csv', 'newsdata_174.csv', 'newsdata_175.csv', 'newsdata_176.csv', 'newsdata_177.csv', 'newsdata_178.csv', 'newsdata_179.csv', 'newsdata_180.csv', 'newsdata_181.csv', 'newsdata_182.csv', 'newsdata_183.csv', 'newsdata_184.csv', 'newsdata_185.csv', 'newsdata_186.csv', 'newsdata_187.csv', 'newsdata_188.csv', 'newsdata_189.csv', 'newsdata_190.csv', 'newsdata_191.csv', 'newsdata_192.csv', 'newsdata_193.csv', 'newsdata_194.csv', 'newsdata_195.csv', 'newsdata_196.csv', 'newsdata_197.csv', 'newsdata_198.csv', 'newsdata_199.csv', 'newsdata_200.csv', 'newsdata_201.csv', 'newsdata_202.csv', 'newsdata_203.csv', 'newsdata_204.csv', 'newsdata_205.csv', 'newsdata_206.csv', 'newsdata_207.csv', 'newsdata_208.csv', 'newsdata_209.csv', 'newsdata_210.csv', 'newsdata_211.csv', 'newsdata_212.csv', 'newsdata_213.csv',\n",
    "'newsdata_214.csv', 'newsdata_215.csv', 'newsdata_216.csv', 'newsdata_217.csv', 'newsdata_218.csv', 'newsdata_219.csv', 'newsdata_220.csv', 'newsdata_221.csv', 'newsdata_222.csv', 'newsdata_223.csv', 'newsdata_224.csv', 'newsdata_225.csv', 'newsdata_226.csv', 'newsdata_227.csv', 'newsdata_228.csv', 'newsdata_229.csv', 'newsdata_230.csv', 'newsdata_231.csv', 'newsdata_232.csv', 'newsdata_233.csv', 'newsdata_234.csv', 'newsdata_235.csv', 'newsdata_236.csv', 'newsdata_237.csv', 'newsdata_238.csv', 'newsdata_239.csv','newsdata_241.csv', 'newsdata_242.csv', 'newsdata_243.csv', 'newsdata_244.csv', 'newsdata_245.csv', 'newsdata_246.csv', 'newsdata_247.csv', 'newsdata_248.csv', 'newsdata_249.csv', 'newsdata_251.csv', 'newsdata_252.csv', 'newsdata_253.csv', 'newsdata_254.csv', 'newsdata_255.csv', 'newsdata_256.csv', 'newsdata_257.csv', 'newsdata_258.csv', 'newsdata_259.csv', 'newsdata_260.csv', 'newsdata_261.csv', 'newsdata_262.csv', 'newsdata_263.csv', 'newsdata_264.csv', 'newsdata_265.csv', 'newsdata_266.csv', 'newsdata_267.csv', 'newsdata_268.csv', 'newsdata_269.csv', 'newsdata_270.csv', 'newsdata_271.csv', 'newsdata_272.csv', 'newsdata_273.csv', 'newsdata_274.csv', 'newsdata_275.csv', 'newsdata_276.csv', 'newsdata_277.csv', 'newsdata_278.csv', 'newsdata_279.csv', 'newsdata_280.csv', 'newsdata_281.csv', 'newsdata_282.csv', 'newsdata_283.csv', 'newsdata_284.csv',\n",
    "'newsdata_285.csv', 'newsdata_286.csv', 'newsdata_287.csv', 'newsdata_288.csv', 'newsdata_289.csv', 'newsdata_290.csv', 'newsdata_291.csv', 'newsdata_292.csv', 'newsdata_293.csv', 'newsdata_294.csv', 'newsdata_295.csv', 'newsdata_296.csv', 'newsdata_297.csv', 'newsdata_298.csv', 'newsdata_299.csv', 'newsdata_300.csv', 'newsdata_301.csv', 'newsdata_302.csv', 'newsdata_303.csv', 'newsdata_304.csv', 'newsdata_305.csv', 'newsdata_306.csv', 'newsdata_307.csv', 'newsdata_308.csv', 'newsdata_309.csv', 'newsdata_310.csv', 'newsdata_311.csv', 'newsdata_312.csv', 'newsdata_313.csv', 'newsdata_314.csv', 'newsdata_315.csv', 'newsdata_316.csv', 'newsdata_317.csv', 'newsdata_318.csv', 'newsdata_319.csv', 'newsdata_320.csv', 'newsdata_321.csv', 'newsdata_322.csv', 'newsdata_323.csv', 'newsdata_324.csv', 'newsdata_325.csv', 'newsdata_326.csv', 'newsdata_327.csv', 'newsdata_328.csv', 'newsdata_329.csv', 'newsdata_329.csv', 'newsdata_330.csv', 'newsdata_331.csv', 'newsdata_332.csv', 'newsdata_333.csv', 'newsdata_334.csv', 'newsdata_335.csv', 'newsdata_336.csv', 'newsdata_337.csv', 'newsdata_338.csv', 'newsdata_339.csv', 'newsdata_340.csv', 'newsdata_341.csv', 'newsdata_342.csv', 'newsdata_343.csv', 'newsdata_344.csv', 'newsdata_345.csv', 'newsdata_346.csv', 'newsdata_347.csv', 'newsdata_348.csv', 'newsdata_349.csv', 'newsdata_350.csv', 'newsdata_351.csv', 'newsdata_352.csv', 'newsdata_353.csv', 'newsdata_354.csv',\n",
    "'newsdata_355.csv', 'newsdata_356.csv', 'newsdata_357.csv', 'newsdata_358.csv', 'newsdata_359.csv', 'newsdata_360.csv', 'newsdata_361.csv', 'newsdata_362.csv', 'newsdata_363.csv', 'newsdata_364.csv', 'newsdata_365.csv', 'newsdata_366.csv', 'newsdata_367.csv', 'newsdata_368.csv', 'newsdata_369.csv', 'newsdata_370.csv', 'newsdata_371.csv', 'newsdata_372.csv', 'newsdata_373.csv', 'newsdata_374.csv', 'newsdata_375.csv', 'newsdata_376.csv', 'newsdata_377.csv', 'newsdata_378.csv', 'newsdata_379.csv', 'newsdata_380.csv', 'newsdata_381.csv', 'newsdata_383.csv', 'newsdata_384.csv', 'newsdata_385.csv', 'newsdata_386.csv', 'newsdata_387.csv', 'newsdata_388.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         year                                              title  \\\n",
      "0        2016  Colts GM Ryan Grigson says Andrew Luck's contr...   \n",
      "1        2018       Trump denies report he ordered Mueller fired   \n",
      "2        2019  France's Sarkozy reveals his 'Passions' but in...   \n",
      "3        2016  Paris Hilton: Woman In Black For Uncle Monty's...   \n",
      "4        2019  ECB's Coeure: If we decide to cut rates, we'd ...   \n",
      "...       ...                                                ...   \n",
      "1597455  2019  Merkel ally under police protection after 'Hei...   \n",
      "1597456  2019  Ugandan shilling weakens on uptick in dollar a...   \n",
      "1597457  2019  China's factory prices post steepest fall in t...   \n",
      "1597458  2019  ECB's Draghi warns of bubble risk in the euro ...   \n",
      "1597459  2019  Russia's Yandex releases rival to China's TikT...   \n",
      "\n",
      "                                                   article       publication  \n",
      "0         The Indianapolis Colts made Andrew Luck the h...  Business Insider  \n",
      "1        DAVOS, Switzerland (Reuters) - U.S. President ...           Reuters  \n",
      "2        PARIS (Reuters) - Former French president Nico...           Reuters  \n",
      "3        Paris Hilton arrived at LAX Wednesday dressed ...               TMZ  \n",
      "4        BERLIN, June 17 (Reuters) - ECB board member B...           Reuters  \n",
      "...                                                    ...               ...  \n",
      "1597455  BERLIN (Reuters) - German police said on Monda...           Reuters  \n",
      "1597456  KAMPALA, Oct 14 (Reuters) - The Ugandan shilli...           Reuters  \n",
      "1597457  BEIJING (Reuters) - China’s factory gate price...           Reuters  \n",
      "1597458  WASHINGTON (Reuters) - There are “mild signs” ...           Reuters  \n",
      "1597459  MOSCOW (Reuters) - Russian internet firm Yande...           Reuters  \n",
      "\n",
      "[1597460 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"files\"\n",
    "\n",
    "# Create an empty list to store individual dataframes\n",
    "dfs = []\n",
    "\n",
    "# Iterate over the file names\n",
    "for file_name in file_names:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    \n",
    "    # Read each file as a dataframe\n",
    "    df = pd.read_csv(file_path)  # Modify the read function according to your file format\n",
    "    \n",
    "    # Append the dataframe to the list\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate the list of dataframes into a single dataframe\n",
    "newsdata_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Print the combined dataframe\n",
    "print(newsdata_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1597460, 4)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsdata_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1597460 entries, 0 to 1597459\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count    Dtype \n",
      "---  ------       --------------    ----- \n",
      " 0   year         1597460 non-null  int64 \n",
      " 1   title        1597460 non-null  object\n",
      " 2   article      1597460 non-null  object\n",
      " 3   publication  1597460 non-null  object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 48.8+ MB\n"
     ]
    }
   ],
   "source": [
    "newsdata_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>article</th>\n",
       "      <th>publication</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016</td>\n",
       "      <td>Colts GM Ryan Grigson says Andrew Luck's contr...</td>\n",
       "      <td>The Indianapolis Colts made Andrew Luck the h...</td>\n",
       "      <td>Business Insider</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018</td>\n",
       "      <td>Trump denies report he ordered Mueller fired</td>\n",
       "      <td>DAVOS, Switzerland (Reuters) - U.S. President ...</td>\n",
       "      <td>Reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019</td>\n",
       "      <td>France's Sarkozy reveals his 'Passions' but in...</td>\n",
       "      <td>PARIS (Reuters) - Former French president Nico...</td>\n",
       "      <td>Reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016</td>\n",
       "      <td>Paris Hilton: Woman In Black For Uncle Monty's...</td>\n",
       "      <td>Paris Hilton arrived at LAX Wednesday dressed ...</td>\n",
       "      <td>TMZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019</td>\n",
       "      <td>ECB's Coeure: If we decide to cut rates, we'd ...</td>\n",
       "      <td>BERLIN, June 17 (Reuters) - ECB board member B...</td>\n",
       "      <td>Reuters</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year                                              title  \\\n",
       "0  2016  Colts GM Ryan Grigson says Andrew Luck's contr...   \n",
       "1  2018       Trump denies report he ordered Mueller fired   \n",
       "2  2019  France's Sarkozy reveals his 'Passions' but in...   \n",
       "3  2016  Paris Hilton: Woman In Black For Uncle Monty's...   \n",
       "4  2019  ECB's Coeure: If we decide to cut rates, we'd ...   \n",
       "\n",
       "                                             article       publication  \n",
       "0   The Indianapolis Colts made Andrew Luck the h...  Business Insider  \n",
       "1  DAVOS, Switzerland (Reuters) - U.S. President ...           Reuters  \n",
       "2  PARIS (Reuters) - Former French president Nico...           Reuters  \n",
       "3  Paris Hilton arrived at LAX Wednesday dressed ...               TMZ  \n",
       "4  BERLIN, June 17 (Reuters) - ECB board member B...           Reuters  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsdata_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reuters               410\n",
      "The New York Times    178\n",
      "Name: publication, dtype: int64\n",
      "588\n"
     ]
    }
   ],
   "source": [
    "#show me the distribution of rows over publications\n",
    "print(df['publication'].value_counts())\n",
    "#sum up the number of rows per publication\n",
    "print(df['publication'].value_counts().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#renew the index\n",
    "newsdata_df = newsdata_df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>article</th>\n",
       "      <th>publication</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1597455</th>\n",
       "      <td>2019</td>\n",
       "      <td>Merkel ally under police protection after 'Hei...</td>\n",
       "      <td>BERLIN (Reuters) - German police said on Monda...</td>\n",
       "      <td>Reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597456</th>\n",
       "      <td>2019</td>\n",
       "      <td>Ugandan shilling weakens on uptick in dollar a...</td>\n",
       "      <td>KAMPALA, Oct 14 (Reuters) - The Ugandan shilli...</td>\n",
       "      <td>Reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597457</th>\n",
       "      <td>2019</td>\n",
       "      <td>China's factory prices post steepest fall in t...</td>\n",
       "      <td>BEIJING (Reuters) - China’s factory gate price...</td>\n",
       "      <td>Reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597458</th>\n",
       "      <td>2019</td>\n",
       "      <td>ECB's Draghi warns of bubble risk in the euro ...</td>\n",
       "      <td>WASHINGTON (Reuters) - There are “mild signs” ...</td>\n",
       "      <td>Reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597459</th>\n",
       "      <td>2019</td>\n",
       "      <td>Russia's Yandex releases rival to China's TikT...</td>\n",
       "      <td>MOSCOW (Reuters) - Russian internet firm Yande...</td>\n",
       "      <td>Reuters</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         year                                              title  \\\n",
       "1597455  2019  Merkel ally under police protection after 'Hei...   \n",
       "1597456  2019  Ugandan shilling weakens on uptick in dollar a...   \n",
       "1597457  2019  China's factory prices post steepest fall in t...   \n",
       "1597458  2019  ECB's Draghi warns of bubble risk in the euro ...   \n",
       "1597459  2019  Russia's Yandex releases rival to China's TikT...   \n",
       "\n",
       "                                                   article publication  \n",
       "1597455  BERLIN (Reuters) - German police said on Monda...     Reuters  \n",
       "1597456  KAMPALA, Oct 14 (Reuters) - The Ugandan shilli...     Reuters  \n",
       "1597457  BEIJING (Reuters) - China’s factory gate price...     Reuters  \n",
       "1597458  WASHINGTON (Reuters) - There are “mild signs” ...     Reuters  \n",
       "1597459  MOSCOW (Reuters) - Russian internet firm Yande...     Reuters  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsdata_df .tail()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Cleaning text\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove non-alphanumeric characters and extra whitespace\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # Tokenize the article into words\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Lemmatize tokens \n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Join the tokens back into a cleaned article\n",
    "    cleaned_text = ' '.join(tokens)\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the text\n",
    "df['content'] = df['content'].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Bag of words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow (df,column):\n",
    "    vect_bow = CountVectorizer()\n",
    "    vect_bow.fit(df[column])\n",
    "    bow_matrix = vect_bow.transform(df[column])\n",
    "    return bow_matrix, bow_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow(df, \"article\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique values in the bow_matrix\n",
    "unique_values = np.unique(bow_matrix.data)\n",
    "\n",
    "# Print the unique values\n",
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = vect_bow.get_feature_names_out()\n",
    "\n",
    "# Find the words with the desired count\n",
    "target_count = 189\n",
    "target_words = [word for word, count in zip(vocabulary, bow_matrix.sum(axis=0).tolist()[0]) if count == target_count]\n",
    "\n",
    "# Print the words\n",
    "print(target_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selfword2vec (df, column):\n",
    "    # Prepare the data for training the Word2Vec model\n",
    "    sentences = [article.split() for article in df[column]]\n",
    "    \n",
    "    # Train the Word2Vec model\n",
    "    word2vec_df = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "    \n",
    "    return word2vec_df\n",
    "\n",
    "#selfword2vec(df, \"article\", \"contract\")\n",
    "\n",
    "#eotf2vec_df is the word2vec model that can be used for further steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gensim.downloader as api\n",
    "\n",
    "#wv = api.load('word2vec-google-news-300')\n",
    "\n",
    "#pretrained word2vec model pre-trained on entire google news dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/CBS/articles1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['Unnamed: 0', 'id', 'title', \"author\", \"date\", \"year\", \"month\", \"url\"]\n",
    "df = df.drop(columns_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Breitbart           23781\n",
       "CNN                 11488\n",
       "New York Times       7803\n",
       "Business Insider     6757\n",
       "Atlantic              171\n",
       "Name: publication, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"publication\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df[df[\"publication\"]== \"Atlantic\"].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['publication'] = df['publication'].map({'Breitbart': 0, 'CNN': 1, 'New York Times':2, \"Business Insider\":3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'] = df['content'].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((31890, 2), (7973, 2), (9966, 2))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = train_test_split(df, test_size=0.2, stratify=df['publication'])\n",
    "train, validation = train_test_split(train, test_size=0.2, stratify=train['publication'])\n",
    "\n",
    "train.shape, validation.shape, test.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here only for wob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (train['content'].values, train['publication'].values)).batch(batch_size=32)\n",
    "\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (validation['content'].values, validation['publication'].values)).batch(batch_size=32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN with Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.preprocessing.text_vectorization.TextVectorization at 0x1be367ceda0>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use TextVectorization for embedding instead of CountVectorizer as it can be added as a layer to the NN\n",
    "tf.keras.layers.TextVectorization(\n",
    "    max_tokens=None,\n",
    "    standardize='lower_and_strip_punctuation',\n",
    "    split='whitespace',\n",
    "    ngrams=None,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=None,\n",
    "    pad_to_max_tokens=False,\n",
    "    vocabulary=None,\n",
    "    idf_weights=None,\n",
    "    sparse=False,\n",
    "    ragged=False,\n",
    "    encoding='utf-8',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "\n",
    "count_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='count'\n",
    ")\n",
    "\n",
    "count_vectorizer.adapt(train['content'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\johan\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(None, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "UnimplementedError",
     "evalue": "Graph execution error:\n\nDetected at node 'sequential_2/Cast' defined at (most recent call last):\n    File \"c:\\Users\\johan\\anaconda3\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Users\\johan\\anaconda3\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\johan\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\johan\\AppData\\Roaming\\Python\\Python310\\site-packages\\traitlets\\config\\application.py\", line 978, in launch_instance\n      app.start()\n    File \"C:\\Users\\johan\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"C:\\Users\\johan\\AppData\\Roaming\\Python\\Python310\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\johan\\anaconda3\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"c:\\Users\\johan\\anaconda3\\lib\\asyncio\\base_events.py\", line 1906, in _run_once\n      handle._run()\n    File \"c:\\Users\\johan\\anaconda3\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\johan\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\johan\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\johan\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"C:\\Users\\johan\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\johan\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\johan\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\johan\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 2885, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\johan\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 2940, in _run_cell\n      return runner(coro)\n    File \"C:\\Users\\johan\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\johan\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3139, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\johan\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3318, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\johan\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3378, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\johan\\AppData\\Local\\Temp\\ipykernel_15528\\426877864.py\", line 20, in <module>\n      history = model.fit(\n    File \"c:\\Users\\johan\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\johan\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"c:\\Users\\johan\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\johan\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\johan\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"c:\\Users\\johan\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1050, in train_step\n      y_pred = self(x, training=True)\n    File \"c:\\Users\\johan\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\johan\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 558, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"c:\\Users\\johan\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\johan\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\johan\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\johan\\anaconda3\\lib\\site-packages\\keras\\engine\\sequential.py\", line 412, in call\n      return super().call(inputs, training=training, mask=mask)\n    File \"c:\\Users\\johan\\anaconda3\\lib\\site-packages\\keras\\engine\\functional.py\", line 512, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"c:\\Users\\johan\\anaconda3\\lib\\site-packages\\keras\\engine\\functional.py\", line 651, in _run_internal_graph\n      y = self._conform_to_reference_input(y, ref_input=x)\n    File \"c:\\Users\\johan\\anaconda3\\lib\\site-packages\\keras\\engine\\functional.py\", line 748, in _conform_to_reference_input\n      tensor = tf.cast(tensor, dtype=ref_input.dtype)\nNode: 'sequential_2/Cast'\nCast string to float is not supported\n\t [[{{node sequential_2/Cast}}]] [Op:__inference_train_function_3493]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32mc:\\CBS\\NLP_finalproject\\starting_point_Johannes_new.ipynb Cell 43\u001b[0m in \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/CBS/NLP_finalproject/starting_point_Johannes_new.ipynb#X56sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m Sequential([\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/CBS/NLP_finalproject/starting_point_Johannes_new.ipynb#X56sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mEmbedding(input_dim\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(word2vec_df\u001b[39m.\u001b[39mwv) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/CBS/NLP_finalproject/starting_point_Johannes_new.ipynb#X56sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                               output_dim\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/CBS/NLP_finalproject/starting_point_Johannes_new.ipynb#X56sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     layers\u001b[39m.\u001b[39mDense(\u001b[39m1\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msigmoid\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/CBS/NLP_finalproject/starting_point_Johannes_new.ipynb#X56sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m ])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/CBS/NLP_finalproject/starting_point_Johannes_new.ipynb#X56sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/CBS/NLP_finalproject/starting_point_Johannes_new.ipynb#X56sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     loss\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlosses\u001b[39m.\u001b[39mcategorical_crossentropy,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/CBS/NLP_finalproject/starting_point_Johannes_new.ipynb#X56sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     optimizer\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mAdam(learning_rate\u001b[39m=\u001b[39m\u001b[39m0.0001\u001b[39m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/CBS/NLP_finalproject/starting_point_Johannes_new.ipynb#X56sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/CBS/NLP_finalproject/starting_point_Johannes_new.ipynb#X56sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m )\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/CBS/NLP_finalproject/starting_point_Johannes_new.ipynb#X56sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mfit(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/CBS/NLP_finalproject/starting_point_Johannes_new.ipynb#X56sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     training_dataset,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/CBS/NLP_finalproject/starting_point_Johannes_new.ipynb#X56sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(training_dataset),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/CBS/NLP_finalproject/starting_point_Johannes_new.ipynb#X56sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     epochs\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/CBS/NLP_finalproject/starting_point_Johannes_new.ipynb#X56sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     validation_data\u001b[39m=\u001b[39mvalidation_dataset,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/CBS/NLP_finalproject/starting_point_Johannes_new.ipynb#X56sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     validation_steps\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(validation_dataset)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/CBS/NLP_finalproject/starting_point_Johannes_new.ipynb#X56sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\johan\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\johan\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mUnimplementedError\u001b[0m: Graph execution error:\n\nDetected at node 'sequential_2/Cast' defined at (most recent call last):\n    File \"c:\\Users\\johan\\anaconda3\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Users\\johan\\anaconda3\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\johan\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\johan\\AppData\\Roaming\\Python\\Python310\\site-packages\\traitlets\\config\\application.py\", line 978, in launch_instance\n      app.start()\n    File \"C:\\Users\\johan\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"C:\\Users\\johan\\AppData\\Roaming\\Python\\Python310\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\johan\\anaconda3\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"c:\\Users\\johan\\anaconda3\\lib\\asyncio\\base_events.py\", line 1906, in _run_once\n      handle._run()\n    File \"c:\\Users\\johan\\anaconda3\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\johan\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\johan\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\johan\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"C:\\Users\\johan\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\johan\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\johan\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\johan\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 2885, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\johan\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 2940, in _run_cell\n      return runner(coro)\n    File \"C:\\Users\\johan\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\johan\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3139, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\johan\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3318, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\johan\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3378, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\johan\\AppData\\Local\\Temp\\ipykernel_15528\\426877864.py\", line 20, in <module>\n      history = model.fit(\n    File \"c:\\Users\\johan\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\johan\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"c:\\Users\\johan\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\johan\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\johan\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"c:\\Users\\johan\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1050, in train_step\n      y_pred = self(x, training=True)\n    File \"c:\\Users\\johan\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\johan\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 558, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"c:\\Users\\johan\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\johan\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\johan\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\johan\\anaconda3\\lib\\site-packages\\keras\\engine\\sequential.py\", line 412, in call\n      return super().call(inputs, training=training, mask=mask)\n    File \"c:\\Users\\johan\\anaconda3\\lib\\site-packages\\keras\\engine\\functional.py\", line 512, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"c:\\Users\\johan\\anaconda3\\lib\\site-packages\\keras\\engine\\functional.py\", line 651, in _run_internal_graph\n      y = self._conform_to_reference_input(y, ref_input=x)\n    File \"c:\\Users\\johan\\anaconda3\\lib\\site-packages\\keras\\engine\\functional.py\", line 748, in _conform_to_reference_input\n      tensor = tf.cast(tensor, dtype=ref_input.dtype)\nNode: 'sequential_2/Cast'\nCast string to float is not supported\n\t [[{{node sequential_2/Cast}}]] [Op:__inference_train_function_3493]"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    count_vectorizer,\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.categorical_crossentropy,\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    training_dataset,\n",
    "    steps_per_epoch=len(training_dataset),\n",
    "    epochs=5,\n",
    "    validation_data=validation_dataset,\n",
    "    validation_steps=len(validation_dataset)\n",
    ")\n",
    "\n",
    "#Hyperparameters are optimized according to random search for df!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312/312 [==============================] - 8s 9ms/step\n",
      "Accuracy: 0.47340959261489063\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "predictions = model.predict(test['content']).round()\n",
    "accuracy = accuracy_score(test['publication'].values, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 08m 52s]\n",
      "val_accuracy: 0.9115347464879354\n",
      "\n",
      "Best val_accuracy So Far: 0.9156737128893534\n",
      "Total elapsed time: 01h 05m 33s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "import kerastuner as kt\n",
    "\n",
    "def build_model(hp):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(count_vectorizer)\n",
    "\n",
    "    # Tune the number of units in the dense layers\n",
    "    hp_units = hp.Int('units', min_value=32, max_value=128, step=32)\n",
    "    model.add(layers.Dense(units=hp_units, activation='relu'))\n",
    "    model.add(layers.Dropout(hp.Float('dropout_1', 0, 0.5, step=0.1)))\n",
    "\n",
    "    model.add(layers.Dense(units=hp_units, activation='relu'))\n",
    "    model.add(layers.Dropout(hp.Float('dropout_2', 0, 0.5, step=0.1)))\n",
    "    model.add(layers.Dense(4, activation='softmax'))\n",
    "    \n",
    "\n",
    "    # Tune the learning rate for the optimizer\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=hp_learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5,\n",
    "    executions_per_trial=3,\n",
    "    directory='my_dir',\n",
    "    project_name='tuning'\n",
    ")\n",
    "\n",
    "tuner.search(training_dataset,\n",
    "             validation_data=validation_dataset,\n",
    "             epochs=5,\n",
    "             validation_steps=len(validation_dataset))\n",
    "\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "best_hyperparameters = tuner.get_best_hyperparameters(1)[0]\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\")\n",
    "for param, value in best_hyperparameters.values.items():\n",
    "    print(f\"{param}: {value}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN with Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_size = 32\n",
    "#\n",
    "#training_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "#    (train['content'].values, train['publication'].values)).batch(batch_size)\n",
    "#\n",
    "#validation_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "#    (validation['content'].values, validation['publication'].values)).batch(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_df = selfword2vec(df, \"content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the maximum length of the input sequences\n",
    "max_length = max(len(seq) for seq in training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\johan\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(None, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "997/997 [==============================] - 12s 7ms/step - loss: 0.0000e+00 - accuracy: 0.4853 - val_loss: 0.0000e+00 - val_accuracy: 0.4856\n",
      "Epoch 2/5\n",
      "997/997 [==============================] - 8s 6ms/step - loss: 0.0000e+00 - accuracy: 0.4859 - val_loss: 0.0000e+00 - val_accuracy: 0.4879\n",
      "Epoch 3/5\n",
      "997/997 [==============================] - 6s 4ms/step - loss: 0.0000e+00 - accuracy: 0.4865 - val_loss: 0.0000e+00 - val_accuracy: 0.4878\n",
      "Epoch 4/5\n",
      "997/997 [==============================] - 5s 4ms/step - loss: 0.0000e+00 - accuracy: 0.4864 - val_loss: 0.0000e+00 - val_accuracy: 0.4876\n",
      "Epoch 5/5\n",
      "997/997 [==============================] - 4s 4ms/step - loss: 0.0000e+00 - accuracy: 0.4865 - val_loss: 0.0000e+00 - val_accuracy: 0.4876\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Tokenize the text sequences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['content'])\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index) + 1\n",
    "\n",
    "# Convert text sequences to integer sequences\n",
    "train_sequences = tokenizer.texts_to_sequences(train['content'])\n",
    "validation_sequences = tokenizer.texts_to_sequences(validation['content'])\n",
    "\n",
    "# Pad sequences to ensure equal length\n",
    "max_length = 100  # Choose the desired maximum length for sequences\n",
    "train_data = pad_sequences(train_sequences, maxlen=max_length, padding='post')\n",
    "validation_data = pad_sequences(validation_sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Update the training and validation datasets\n",
    "training_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (train_data, train['publication'].values)).batch(batch_size)\n",
    "\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (validation_data, validation['publication'].values)).batch(batch_size)\n",
    "\n",
    "# Update the input shape in the model\n",
    "model_w2v = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=vocab_size,\n",
    "                              output_dim=100,\n",
    "                              weights=[np.vstack((np.zeros(100), word2vec_df.wv.vectors))],\n",
    "                              trainable=False,\n",
    "                              input_length=max_length),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_w2v.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model_w2v.fit(\n",
    "    training_dataset,\n",
    "    epochs=5,\n",
    "    validation_data=validation_dataset\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN w2v optimizatin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Tuner from my_dir\\tuning\\tuner0.json\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.core.dense.Dense object at 0x00000182E975A200> and <keras.layers.pooling.global_average_pooling1d.GlobalAveragePooling1D object at 0x00000182E9758C10>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.core.dense.Dense object at 0x00000182E3A34640> and <keras.layers.core.dense.Dense object at 0x00000182E975A200>).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Received incompatible tensor with shape (96,) when attempting to restore variable with shape (64,) and name dense/bias:0.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\CBS\\NLP_finalproject\\starting_point_Johannes_new.ipynb Cell 52\u001b[0m in \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/CBS/NLP_finalproject/starting_point_Johannes_new.ipynb#Y125sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m tuner \u001b[39m=\u001b[39m kt\u001b[39m.\u001b[39mRandomSearch(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/CBS/NLP_finalproject/starting_point_Johannes_new.ipynb#Y125sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     build_model,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/CBS/NLP_finalproject/starting_point_Johannes_new.ipynb#Y125sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     objective\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_accuracy\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/CBS/NLP_finalproject/starting_point_Johannes_new.ipynb#Y125sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     project_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtuning\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/CBS/NLP_finalproject/starting_point_Johannes_new.ipynb#Y125sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/CBS/NLP_finalproject/starting_point_Johannes_new.ipynb#Y125sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m tuner\u001b[39m.\u001b[39msearch(training_dataset,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/CBS/NLP_finalproject/starting_point_Johannes_new.ipynb#Y125sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m              validation_data\u001b[39m=\u001b[39mvalidation_dataset,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/CBS/NLP_finalproject/starting_point_Johannes_new.ipynb#Y125sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m              epochs\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/CBS/NLP_finalproject/starting_point_Johannes_new.ipynb#Y125sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m              validation_steps\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(validation_dataset))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/CBS/NLP_finalproject/starting_point_Johannes_new.ipynb#Y125sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m best_model \u001b[39m=\u001b[39m tuner\u001b[39m.\u001b[39mget_best_models(num_models\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/CBS/NLP_finalproject/starting_point_Johannes_new.ipynb#Y125sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m best_hyperparameters \u001b[39m=\u001b[39m tuner\u001b[39m.\u001b[39mget_best_hyperparameters(\u001b[39m1\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/CBS/NLP_finalproject/starting_point_Johannes_new.ipynb#Y125sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39m# Print the best hyperparameters\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\johan\\anaconda3\\lib\\site-packages\\keras_tuner\\engine\\tuner.py:366\u001b[0m, in \u001b[0;36mTuner.get_best_models\u001b[1;34m(self, num_models)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[39m\"\"\"Returns the best model(s), as determined by the tuner's objective.\u001b[39;00m\n\u001b[0;32m    349\u001b[0m \n\u001b[0;32m    350\u001b[0m \u001b[39mThe models are loaded with the weights corresponding to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[39m    List of trained model instances sorted from the best to the worst.\u001b[39;00m\n\u001b[0;32m    364\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    365\u001b[0m \u001b[39m# Method only exists in this class for the docstring override.\u001b[39;00m\n\u001b[1;32m--> 366\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mget_best_models(num_models)\n",
      "File \u001b[1;32mc:\\Users\\johan\\anaconda3\\lib\\site-packages\\keras_tuner\\engine\\base_tuner.py:364\u001b[0m, in \u001b[0;36mBaseTuner.get_best_models\u001b[1;34m(self, num_models)\u001b[0m\n\u001b[0;32m    349\u001b[0m \u001b[39m\"\"\"Returns the best model(s), as determined by the objective.\u001b[39;00m\n\u001b[0;32m    350\u001b[0m \n\u001b[0;32m    351\u001b[0m \u001b[39mThis method is for querying the models trained during the search.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[39m    List of trained models sorted from the best to the worst.\u001b[39;00m\n\u001b[0;32m    362\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    363\u001b[0m best_trials \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moracle\u001b[39m.\u001b[39mget_best_trials(num_models)\n\u001b[1;32m--> 364\u001b[0m models \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload_model(trial) \u001b[39mfor\u001b[39;00m trial \u001b[39min\u001b[39;00m best_trials]\n\u001b[0;32m    365\u001b[0m \u001b[39mreturn\u001b[39;00m models\n",
      "File \u001b[1;32mc:\\Users\\johan\\anaconda3\\lib\\site-packages\\keras_tuner\\engine\\base_tuner.py:364\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    349\u001b[0m \u001b[39m\"\"\"Returns the best model(s), as determined by the objective.\u001b[39;00m\n\u001b[0;32m    350\u001b[0m \n\u001b[0;32m    351\u001b[0m \u001b[39mThis method is for querying the models trained during the search.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[39m    List of trained models sorted from the best to the worst.\u001b[39;00m\n\u001b[0;32m    362\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    363\u001b[0m best_trials \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moracle\u001b[39m.\u001b[39mget_best_trials(num_models)\n\u001b[1;32m--> 364\u001b[0m models \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload_model(trial) \u001b[39mfor\u001b[39;00m trial \u001b[39min\u001b[39;00m best_trials]\n\u001b[0;32m    365\u001b[0m \u001b[39mreturn\u001b[39;00m models\n",
      "File \u001b[1;32mc:\\Users\\johan\\anaconda3\\lib\\site-packages\\keras_tuner\\engine\\tuner.py:297\u001b[0m, in \u001b[0;36mTuner.load_model\u001b[1;34m(self, trial)\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[39m# Reload best checkpoint.\u001b[39;00m\n\u001b[0;32m    295\u001b[0m \u001b[39m# Only load weights to avoid loading `custom_objects`.\u001b[39;00m\n\u001b[0;32m    296\u001b[0m \u001b[39mwith\u001b[39;00m maybe_distribute(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribution_strategy):\n\u001b[1;32m--> 297\u001b[0m     model\u001b[39m.\u001b[39;49mload_weights(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_checkpoint_fname(trial\u001b[39m.\u001b[39;49mtrial_id))\n\u001b[0;32m    298\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\johan\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\johan\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:720\u001b[0m, in \u001b[0;36mBaseResourceVariable._restore_from_tensors\u001b[1;34m(self, restored_tensors)\u001b[0m\n\u001b[0;32m    717\u001b[0m   assigned_variable \u001b[39m=\u001b[39m shape_safe_assign_variable_handle(\n\u001b[0;32m    718\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape, restored_tensor)\n\u001b[0;32m    719\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 720\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    721\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mReceived incompatible tensor with shape \u001b[39m\u001b[39m{\u001b[39;00mrestored_tensor\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    722\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwhen attempting to restore variable with shape \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    723\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mand name \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m    724\u001b[0m \u001b[39mreturn\u001b[39;00m assigned_variable\n",
      "\u001b[1;31mValueError\u001b[0m: Received incompatible tensor with shape (96,) when attempting to restore variable with shape (64,) and name dense/bias:0."
     ]
    }
   ],
   "source": [
    "import kerastuner as kt\n",
    "\n",
    "def build_model(hp):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Embedding(input_dim=vocab_size,\n",
    "                                        output_dim=100,\n",
    "                                        weights=[np.vstack((np.zeros(100), word2vec_df.wv.vectors))],\n",
    "                                        trainable=False,\n",
    "                                        input_length=max_length))\n",
    "    model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
    "\n",
    "    # Tune the dropout rate\n",
    "    hp_dropout_rate = hp.Float('dropout_rate', min_value=0.0, max_value=0.5, step=0.1)\n",
    "    model.add(tf.keras.layers.Dropout(rate=hp_dropout_rate))\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5,\n",
    "    executions_per_trial=3,\n",
    "    directory='my_dir',\n",
    "    project_name='tuning'\n",
    ")\n",
    "\n",
    "tuner.search(training_dataset,\n",
    "             validation_data=validation_dataset,\n",
    "             epochs=5,\n",
    "             validation_steps=len(validation_dataset))\n",
    "\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "best_hyperparameters = tuner.get_best_hyperparameters(1)[0]\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\")\n",
    "for param, value in best_hyperparameters.values.items():\n",
    "    print(f\"{param}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
