{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read me"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to start with pre-filtered data set, start with import section and then skip everything until \"3. Starting point\". Start from there"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_hub\n",
      "  Downloading tensorflow_hub-0.13.0-py2.py3-none-any.whl (100 kB)\n",
      "     -------------------------------------- 100.6/100.6 kB 2.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: protobuf>=3.19.6 in c:\\users\\johan\\anaconda3\\lib\\site-packages (from tensorflow_hub) (4.22.1)\n",
      "Requirement already satisfied: numpy>=1.12.0 in c:\\users\\johan\\anaconda3\\lib\\site-packages (from tensorflow_hub) (1.23.5)\n",
      "Installing collected packages: tensorflow_hub\n",
      "Successfully installed tensorflow_hub-0.13.0\n"
     ]
    }
   ],
   "source": [
    "#statements for ucloud\n",
    "#!pip install pandas\n",
    "#!pip install numpy\n",
    "#!pip install seaborn\n",
    "#!pip install nltk\n",
    "#!pip install matplotlib\n",
    "#!pip install scikit-learn\n",
    "#!pip install tensorflow\n",
    "#!pip install tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\johan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\johan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\johan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "C:\\Users\\johan\\AppData\\Local\\Temp\\ipykernel_7520\\1789534113.py:26: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  from kerastuner.tuners import RandomSearch\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import seaborn as sns \n",
    "\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, WordNetLemmatizer\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Starting point"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Reading in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list with the names of the files\n",
    "file_names = ['newsdata_1.csv', 'newsdata_2.csv', 'newsdata_3.csv', 'newsdata_4.csv', 'newsdata_5.csv', 'newsdata_6.csv', 'newsdata_7.csv', 'newsdata_8.csv', 'newsdata_9.csv', 'newsdata_10.csv', 'newsdata_11.csv', 'newsdata_12.csv', 'newsdata_13.csv', 'newsdata_14.csv', 'newsdata_15.csv', 'newsdata_16.csv', 'newsdata_17.csv', 'newsdata_18.csv', 'newsdata_19.csv', 'newsdata_20.csv', 'newsdata_21.csv', 'newsdata_22.csv', 'newsdata_23.csv', 'newsdata_24.csv', 'newsdata_25.csv', 'newsdata_26.csv', 'newsdata_27.csv', 'newsdata_28.csv', 'newsdata_29.csv', 'newsdata_30.csv', 'newsdata_31.csv', 'newsdata_32.csv', 'newsdata_33.csv', 'newsdata_34.csv', 'newsdata_35.csv', 'newsdata_36.csv', 'newsdata_37.csv', 'newsdata_38.csv', 'newsdata_39.csv', 'newsdata_40.csv', 'newsdata_41.csv', 'newsdata_42.csv', 'newsdata_43.csv', 'newsdata_44.csv', 'newsdata_45.csv', 'newsdata_46.csv', 'newsdata_47.csv', 'newsdata_48.csv', 'newsdata_49.csv', 'newsdata_50.csv', 'newsdata_51.csv', 'newsdata_52.csv', 'newsdata_53.csv', 'newsdata_54.csv', 'newsdata_55.csv', 'newsdata_56.csv', 'newsdata_57.csv', 'newsdata_58.csv', 'newsdata_59.csv', 'newsdata_60.csv', 'newsdata_61.csv', 'newsdata_62.csv', 'newsdata_63.csv', 'newsdata_64.csv', 'newsdata_65.csv', 'newsdata_66.csv', 'newsdata_67.csv', 'newsdata_68.csv', 'newsdata_69.csv', 'newsdata_70.csv', 'newsdata_71.csv',\n",
    "'newsdata_72.csv', 'newsdata_73.csv', 'newsdata_74.csv', 'newsdata_75.csv', 'newsdata_76.csv', 'newsdata_77.csv', 'newsdata_78.csv', 'newsdata_79.csv', 'newsdata_80.csv', 'newsdata_81.csv', 'newsdata_82.csv', 'newsdata_83.csv', 'newsdata_84.csv', 'newsdata_85.csv', 'newsdata_86.csv', 'newsdata_87.csv', 'newsdata_88.csv', 'newsdata_89.csv', 'newsdata_90.csv', 'newsdata_91.csv', 'newsdata_92.csv', 'newsdata_93.csv', 'newsdata_94.csv', 'newsdata_95.csv', 'newsdata_96.csv', 'newsdata_97.csv', 'newsdata_98.csv', 'newsdata_99.csv', 'newsdata_100.csv', 'newsdata_101.csv', 'newsdata_102.csv', 'newsdata_103.csv', 'newsdata_104.csv', 'newsdata_105.csv', 'newsdata_106.csv', 'newsdata_107.csv', 'newsdata_108.csv', 'newsdata_109.csv', 'newsdata_110.csv', 'newsdata_111.csv', 'newsdata_112.csv', 'newsdata_113.csv', 'newsdata_114.csv', 'newsdata_115.csv', 'newsdata_116.csv', 'newsdata_117.csv', 'newsdata_118.csv', 'newsdata_119.csv', 'newsdata_120.csv', 'newsdata_121.csv', 'newsdata_122.csv', 'newsdata_123.csv', 'newsdata_124.csv', 'newsdata_125.csv', 'newsdata_126.csv', 'newsdata_127.csv', 'newsdata_128.csv', 'newsdata_129.csv', 'newsdata_130.csv', 'newsdata_131.csv', 'newsdata_132.csv', 'newsdata_133.csv', 'newsdata_134.csv', 'newsdata_135.csv', 'newsdata_136.csv', 'newsdata_137.csv', 'newsdata_138.csv', 'newsdata_139.csv', 'newsdata_140.csv', 'newsdata_141.csv', 'newsdata_142.csv',\n",
    "'newsdata_143.csv', 'newsdata_144.csv', 'newsdata_145.csv', 'newsdata_146.csv', 'newsdata_147.csv', 'newsdata_148.csv', 'newsdata_149.csv', 'newsdata_150.csv', 'newsdata_151.csv', 'newsdata_152.csv', 'newsdata_153.csv', 'newsdata_154.csv', 'newsdata_155.csv', 'newsdata_156.csv', 'newsdata_157.csv', 'newsdata_158.csv', 'newsdata_159.csv', 'newsdata_160.csv', 'newsdata_161.csv', 'newsdata_162.csv', 'newsdata_163.csv', 'newsdata_164.csv', 'newsdata_165.csv', 'newsdata_166.csv', 'newsdata_167.csv', 'newsdata_168.csv', 'newsdata_169.csv', 'newsdata_170.csv', 'newsdata_171.csv', 'newsdata_172.csv', 'newsdata_173.csv', 'newsdata_174.csv', 'newsdata_175.csv', 'newsdata_176.csv', 'newsdata_177.csv', 'newsdata_178.csv', 'newsdata_179.csv', 'newsdata_180.csv', 'newsdata_181.csv', 'newsdata_182.csv', 'newsdata_183.csv', 'newsdata_184.csv', 'newsdata_185.csv', 'newsdata_186.csv', 'newsdata_187.csv', 'newsdata_188.csv', 'newsdata_189.csv', 'newsdata_190.csv', 'newsdata_191.csv', 'newsdata_192.csv', 'newsdata_193.csv', 'newsdata_194.csv', 'newsdata_195.csv', 'newsdata_196.csv', 'newsdata_197.csv', 'newsdata_198.csv', 'newsdata_199.csv', 'newsdata_200.csv', 'newsdata_201.csv', 'newsdata_202.csv', 'newsdata_203.csv', 'newsdata_204.csv', 'newsdata_205.csv', 'newsdata_206.csv', 'newsdata_207.csv', 'newsdata_208.csv', 'newsdata_209.csv', 'newsdata_210.csv', 'newsdata_211.csv', 'newsdata_212.csv', 'newsdata_213.csv',\n",
    "'newsdata_214.csv', 'newsdata_215.csv', 'newsdata_216.csv', 'newsdata_217.csv', 'newsdata_218.csv', 'newsdata_219.csv', 'newsdata_220.csv', 'newsdata_221.csv', 'newsdata_222.csv', 'newsdata_223.csv', 'newsdata_224.csv', 'newsdata_225.csv', 'newsdata_226.csv', 'newsdata_227.csv', 'newsdata_228.csv', 'newsdata_229.csv', 'newsdata_230.csv', 'newsdata_231.csv', 'newsdata_232.csv', 'newsdata_233.csv', 'newsdata_234.csv', 'newsdata_235.csv', 'newsdata_236.csv', 'newsdata_237.csv', 'newsdata_238.csv', 'newsdata_239.csv','newsdata_241.csv', 'newsdata_242.csv', 'newsdata_243.csv', 'newsdata_244.csv', 'newsdata_245.csv', 'newsdata_246.csv', 'newsdata_247.csv', 'newsdata_248.csv', 'newsdata_249.csv', 'newsdata_251.csv', 'newsdata_252.csv', 'newsdata_253.csv', 'newsdata_254.csv', 'newsdata_255.csv', 'newsdata_256.csv', 'newsdata_257.csv', 'newsdata_258.csv', 'newsdata_259.csv', 'newsdata_260.csv', 'newsdata_261.csv', 'newsdata_262.csv', 'newsdata_263.csv', 'newsdata_264.csv', 'newsdata_265.csv', 'newsdata_266.csv', 'newsdata_267.csv', 'newsdata_268.csv', 'newsdata_269.csv', 'newsdata_270.csv', 'newsdata_271.csv', 'newsdata_272.csv', 'newsdata_273.csv', 'newsdata_274.csv', 'newsdata_275.csv', 'newsdata_276.csv', 'newsdata_277.csv', 'newsdata_278.csv', 'newsdata_279.csv', 'newsdata_280.csv', 'newsdata_281.csv', 'newsdata_282.csv', 'newsdata_283.csv', 'newsdata_284.csv',\n",
    "'newsdata_285.csv', 'newsdata_286.csv', 'newsdata_287.csv', 'newsdata_288.csv', 'newsdata_289.csv', 'newsdata_290.csv', 'newsdata_291.csv', 'newsdata_292.csv', 'newsdata_293.csv', 'newsdata_294.csv', 'newsdata_295.csv', 'newsdata_296.csv', 'newsdata_297.csv', 'newsdata_298.csv', 'newsdata_299.csv', 'newsdata_300.csv', 'newsdata_301.csv', 'newsdata_302.csv', 'newsdata_303.csv', 'newsdata_304.csv', 'newsdata_305.csv', 'newsdata_306.csv', 'newsdata_307.csv', 'newsdata_308.csv', 'newsdata_309.csv', 'newsdata_310.csv', 'newsdata_311.csv', 'newsdata_312.csv', 'newsdata_313.csv', 'newsdata_314.csv', 'newsdata_315.csv', 'newsdata_316.csv', 'newsdata_317.csv', 'newsdata_318.csv', 'newsdata_319.csv', 'newsdata_320.csv', 'newsdata_321.csv', 'newsdata_322.csv', 'newsdata_323.csv', 'newsdata_324.csv', 'newsdata_325.csv', 'newsdata_326.csv', 'newsdata_327.csv', 'newsdata_328.csv', 'newsdata_329.csv', 'newsdata_329.csv', 'newsdata_330.csv', 'newsdata_331.csv', 'newsdata_332.csv', 'newsdata_333.csv', 'newsdata_334.csv', 'newsdata_335.csv', 'newsdata_336.csv', 'newsdata_337.csv', 'newsdata_338.csv', 'newsdata_339.csv', 'newsdata_340.csv', 'newsdata_341.csv', 'newsdata_342.csv', 'newsdata_343.csv', 'newsdata_344.csv', 'newsdata_345.csv', 'newsdata_346.csv', 'newsdata_347.csv', 'newsdata_348.csv', 'newsdata_349.csv', 'newsdata_350.csv', 'newsdata_351.csv', 'newsdata_352.csv', 'newsdata_353.csv', 'newsdata_354.csv',\n",
    "'newsdata_355.csv', 'newsdata_356.csv', 'newsdata_357.csv', 'newsdata_358.csv', 'newsdata_359.csv', 'newsdata_360.csv', 'newsdata_361.csv', 'newsdata_362.csv', 'newsdata_363.csv', 'newsdata_364.csv', 'newsdata_365.csv', 'newsdata_366.csv', 'newsdata_367.csv', 'newsdata_368.csv', 'newsdata_369.csv', 'newsdata_370.csv', 'newsdata_371.csv', 'newsdata_372.csv', 'newsdata_373.csv', 'newsdata_374.csv', 'newsdata_375.csv', 'newsdata_376.csv', 'newsdata_377.csv', 'newsdata_378.csv', 'newsdata_379.csv', 'newsdata_380.csv', 'newsdata_381.csv', 'newsdata_383.csv', 'newsdata_384.csv', 'newsdata_385.csv', 'newsdata_386.csv', 'newsdata_387.csv', 'newsdata_388.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         year                                              title  \\\n",
      "0        2016  Colts GM Ryan Grigson says Andrew Luck's contr...   \n",
      "1        2018       Trump denies report he ordered Mueller fired   \n",
      "2        2019  France's Sarkozy reveals his 'Passions' but in...   \n",
      "3        2016  Paris Hilton: Woman In Black For Uncle Monty's...   \n",
      "4        2019  ECB's Coeure: If we decide to cut rates, we'd ...   \n",
      "...       ...                                                ...   \n",
      "1597455  2019  Merkel ally under police protection after 'Hei...   \n",
      "1597456  2019  Ugandan shilling weakens on uptick in dollar a...   \n",
      "1597457  2019  China's factory prices post steepest fall in t...   \n",
      "1597458  2019  ECB's Draghi warns of bubble risk in the euro ...   \n",
      "1597459  2019  Russia's Yandex releases rival to China's TikT...   \n",
      "\n",
      "                                                   article       publication  \n",
      "0         The Indianapolis Colts made Andrew Luck the h...  Business Insider  \n",
      "1        DAVOS, Switzerland (Reuters) - U.S. President ...           Reuters  \n",
      "2        PARIS (Reuters) - Former French president Nico...           Reuters  \n",
      "3        Paris Hilton arrived at LAX Wednesday dressed ...               TMZ  \n",
      "4        BERLIN, June 17 (Reuters) - ECB board member B...           Reuters  \n",
      "...                                                    ...               ...  \n",
      "1597455  BERLIN (Reuters) - German police said on Monda...           Reuters  \n",
      "1597456  KAMPALA, Oct 14 (Reuters) - The Ugandan shilli...           Reuters  \n",
      "1597457  BEIJING (Reuters) - China’s factory gate price...           Reuters  \n",
      "1597458  WASHINGTON (Reuters) - There are “mild signs” ...           Reuters  \n",
      "1597459  MOSCOW (Reuters) - Russian internet firm Yande...           Reuters  \n",
      "\n",
      "[1597460 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"files\"\n",
    "\n",
    "# Create an empty list to store individual dataframes\n",
    "dfs = []\n",
    "\n",
    "# Iterate over the file names\n",
    "for file_name in file_names:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    \n",
    "    # Read each file as a dataframe\n",
    "    df = pd.read_csv(file_path)  # Modify the read function according to your file format\n",
    "    \n",
    "    # Append the dataframe to the list\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate the list of dataframes into a single dataframe\n",
    "newsdata_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Print the combined dataframe\n",
    "print(newsdata_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1597460, 4)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsdata_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1597460 entries, 0 to 1597459\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count    Dtype \n",
      "---  ------       --------------    ----- \n",
      " 0   year         1597460 non-null  int64 \n",
      " 1   title        1597460 non-null  object\n",
      " 2   article      1597460 non-null  object\n",
      " 3   publication  1597460 non-null  object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 48.8+ MB\n"
     ]
    }
   ],
   "source": [
    "newsdata_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>article</th>\n",
       "      <th>publication</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016</td>\n",
       "      <td>Colts GM Ryan Grigson says Andrew Luck's contr...</td>\n",
       "      <td>The Indianapolis Colts made Andrew Luck the h...</td>\n",
       "      <td>Business Insider</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018</td>\n",
       "      <td>Trump denies report he ordered Mueller fired</td>\n",
       "      <td>DAVOS, Switzerland (Reuters) - U.S. President ...</td>\n",
       "      <td>Reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019</td>\n",
       "      <td>France's Sarkozy reveals his 'Passions' but in...</td>\n",
       "      <td>PARIS (Reuters) - Former French president Nico...</td>\n",
       "      <td>Reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016</td>\n",
       "      <td>Paris Hilton: Woman In Black For Uncle Monty's...</td>\n",
       "      <td>Paris Hilton arrived at LAX Wednesday dressed ...</td>\n",
       "      <td>TMZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019</td>\n",
       "      <td>ECB's Coeure: If we decide to cut rates, we'd ...</td>\n",
       "      <td>BERLIN, June 17 (Reuters) - ECB board member B...</td>\n",
       "      <td>Reuters</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year                                              title  \\\n",
       "0  2016  Colts GM Ryan Grigson says Andrew Luck's contr...   \n",
       "1  2018       Trump denies report he ordered Mueller fired   \n",
       "2  2019  France's Sarkozy reveals his 'Passions' but in...   \n",
       "3  2016  Paris Hilton: Woman In Black For Uncle Monty's...   \n",
       "4  2019  ECB's Coeure: If we decide to cut rates, we'd ...   \n",
       "\n",
       "                                             article       publication  \n",
       "0   The Indianapolis Colts made Andrew Luck the h...  Business Insider  \n",
       "1  DAVOS, Switzerland (Reuters) - U.S. President ...           Reuters  \n",
       "2  PARIS (Reuters) - Former French president Nico...           Reuters  \n",
       "3  Paris Hilton arrived at LAX Wednesday dressed ...               TMZ  \n",
       "4  BERLIN, June 17 (Reuters) - ECB board member B...           Reuters  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsdata_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reuters               410\n",
      "The New York Times    178\n",
      "Name: publication, dtype: int64\n",
      "588\n"
     ]
    }
   ],
   "source": [
    "#show me the distribution of rows over publications\n",
    "print(df['publication'].value_counts())\n",
    "#sum up the number of rows per publication\n",
    "print(df['publication'].value_counts().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#renew the index\n",
    "newsdata_df = newsdata_df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>article</th>\n",
       "      <th>publication</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1597455</th>\n",
       "      <td>2019</td>\n",
       "      <td>Merkel ally under police protection after 'Hei...</td>\n",
       "      <td>BERLIN (Reuters) - German police said on Monda...</td>\n",
       "      <td>Reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597456</th>\n",
       "      <td>2019</td>\n",
       "      <td>Ugandan shilling weakens on uptick in dollar a...</td>\n",
       "      <td>KAMPALA, Oct 14 (Reuters) - The Ugandan shilli...</td>\n",
       "      <td>Reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597457</th>\n",
       "      <td>2019</td>\n",
       "      <td>China's factory prices post steepest fall in t...</td>\n",
       "      <td>BEIJING (Reuters) - China’s factory gate price...</td>\n",
       "      <td>Reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597458</th>\n",
       "      <td>2019</td>\n",
       "      <td>ECB's Draghi warns of bubble risk in the euro ...</td>\n",
       "      <td>WASHINGTON (Reuters) - There are “mild signs” ...</td>\n",
       "      <td>Reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597459</th>\n",
       "      <td>2019</td>\n",
       "      <td>Russia's Yandex releases rival to China's TikT...</td>\n",
       "      <td>MOSCOW (Reuters) - Russian internet firm Yande...</td>\n",
       "      <td>Reuters</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         year                                              title  \\\n",
       "1597455  2019  Merkel ally under police protection after 'Hei...   \n",
       "1597456  2019  Ugandan shilling weakens on uptick in dollar a...   \n",
       "1597457  2019  China's factory prices post steepest fall in t...   \n",
       "1597458  2019  ECB's Draghi warns of bubble risk in the euro ...   \n",
       "1597459  2019  Russia's Yandex releases rival to China's TikT...   \n",
       "\n",
       "                                                   article publication  \n",
       "1597455  BERLIN (Reuters) - German police said on Monda...     Reuters  \n",
       "1597456  KAMPALA, Oct 14 (Reuters) - The Ugandan shilli...     Reuters  \n",
       "1597457  BEIJING (Reuters) - China’s factory gate price...     Reuters  \n",
       "1597458  WASHINGTON (Reuters) - There are “mild signs” ...     Reuters  \n",
       "1597459  MOSCOW (Reuters) - Russian internet firm Yande...     Reuters  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsdata_df .tail()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Cleaning text\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove non-alphanumeric characters and extra whitespace\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # Tokenize the article into words\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Lemmatize tokens \n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Join the tokens back into a cleaned article\n",
    "    cleaned_text = ' '.join(tokens)\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the text\n",
    "df['content'] = df['content'].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Bag of words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow (df,column):\n",
    "    vect_bow = CountVectorizer()\n",
    "    vect_bow.fit(df[column])\n",
    "    bow_matrix = vect_bow.transform(df[column])\n",
    "    return bow_matrix, bow_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow(df, \"article\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique values in the bow_matrix\n",
    "unique_values = np.unique(bow_matrix.data)\n",
    "\n",
    "# Print the unique values\n",
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = vect_bow.get_feature_names_out()\n",
    "\n",
    "# Find the words with the desired count\n",
    "target_count = 189\n",
    "target_words = [word for word, count in zip(vocabulary, bow_matrix.sum(axis=0).tolist()[0]) if count == target_count]\n",
    "\n",
    "# Print the words\n",
    "print(target_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selfword2vec (df, column, word):\n",
    "    # Prepare the data for training the Word2Vec model\n",
    "    sentences = [article.split() for article in df[column]]\n",
    "    \n",
    "    # Train the Word2Vec model\n",
    "    word2vec_df = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "    \n",
    "    # Get the word vector for a specific word\n",
    "    word_vector = word2vec_df.wv[word]\n",
    "\n",
    "    # Find similar words to a given word\n",
    "    similar_words = word2vec_df.wv.most_similar(word, topn=5)\n",
    "\n",
    "    print(similar_words)\n",
    "    return word2vec_df\n",
    "\n",
    "selfword2vec(df, \"article\", \"contract\")\n",
    "\n",
    "#eotf2vec_df is the word2vec model that can be used for further steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gensim.downloader as api\n",
    "\n",
    "#wv = api.load('word2vec-google-news-300')\n",
    "\n",
    "#pretrained word2vec model pre-trained on entire google news dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/CBS/articles1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['Unnamed: 0', 'id', 'title', \"author\", \"date\", \"year\", \"month\", \"url\"]\n",
    "df = df.drop(columns_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Breitbart           23781\n",
       "CNN                 11488\n",
       "New York Times       7803\n",
       "Business Insider     6757\n",
       "Atlantic              171\n",
       "Name: publication, dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"publication\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df[df[\"publication\"]== \"Atlantic\"].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['publication'] = df['publication'].map({'Breitbart': 0, 'CNN': 1, 'New York Times':2, \"Business Insider\":3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'] = df['content'].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((31890, 2), (7973, 2), (9966, 2))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = train_test_split(df, test_size=0.2, stratify=df['publication'])\n",
    "train, validation = train_test_split(train, test_size=0.2, stratify=train['publication'])\n",
    "\n",
    "train.shape, validation.shape, test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "publication    0\n",
       "content        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (train['content'].values, train['publication'].values)).batch(batch_size=32)\n",
    "\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (validation['content'].values, validation['publication'].values)).batch(batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.preprocessing.text_vectorization.TextVectorization at 0x1be367ceda0>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use TextVectorization for embedding instead of CountVectorizer as it can be added as a layer to the NN\n",
    "tf.keras.layers.TextVectorization(\n",
    "    max_tokens=None,\n",
    "    standardize='lower_and_strip_punctuation',\n",
    "    split='whitespace',\n",
    "    ngrams=None,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=None,\n",
    "    pad_to_max_tokens=False,\n",
    "    vocabulary=None,\n",
    "    idf_weights=None,\n",
    "    sparse=False,\n",
    "    ragged=False,\n",
    "    encoding='utf-8',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "\n",
    "count_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='count'\n",
    ")\n",
    "\n",
    "count_vectorizer.adapt(train['content'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\johan\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(None, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "997/997 [==============================] - 29s 24ms/step - loss: 0.0000e+00 - accuracy: 0.4721 - val_loss: 0.0000e+00 - val_accuracy: 0.4917\n",
      "Epoch 2/5\n",
      "997/997 [==============================] - 26s 24ms/step - loss: 0.0000e+00 - accuracy: 0.4832 - val_loss: 0.0000e+00 - val_accuracy: 0.4826\n",
      "Epoch 3/5\n",
      "997/997 [==============================] - 26s 23ms/step - loss: 0.0000e+00 - accuracy: 0.4826 - val_loss: 0.0000e+00 - val_accuracy: 0.4811\n",
      "Epoch 4/5\n",
      "997/997 [==============================] - 26s 23ms/step - loss: 0.0000e+00 - accuracy: 0.4812 - val_loss: 0.0000e+00 - val_accuracy: 0.4789\n",
      "Epoch 5/5\n",
      "997/997 [==============================] - 26s 23ms/step - loss: 0.0000e+00 - accuracy: 0.4812 - val_loss: 0.0000e+00 - val_accuracy: 0.4772\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    count_vectorizer,\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.categorical_crossentropy,\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    training_dataset,\n",
    "    steps_per_epoch=len(training_dataset),\n",
    "    epochs=5,\n",
    "    validation_data=validation_dataset,\n",
    "    validation_steps=len(validation_dataset)\n",
    ")\n",
    "\n",
    "#Hyperparameters are optimized according to random search for df!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312/312 [==============================] - 8s 9ms/step\n",
      "Accuracy: 0.47340959261489063\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "predictions = model.predict(test['content']).round()\n",
    "accuracy = accuracy_score(test['publication'].values, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 08m 52s]\n",
      "val_accuracy: 0.9115347464879354\n",
      "\n",
      "Best val_accuracy So Far: 0.9156737128893534\n",
      "Total elapsed time: 01h 05m 33s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "import kerastuner as kt\n",
    "\n",
    "def build_model(hp):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(count_vectorizer)\n",
    "\n",
    "    # Tune the number of units in the dense layers\n",
    "    hp_units = hp.Int('units', min_value=32, max_value=128, step=32)\n",
    "    model.add(layers.Dense(units=hp_units, activation='relu'))\n",
    "    model.add(layers.Dropout(hp.Float('dropout_1', 0, 0.5, step=0.1)))\n",
    "\n",
    "    model.add(layers.Dense(units=hp_units, activation='relu'))\n",
    "    model.add(layers.Dropout(hp.Float('dropout_2', 0, 0.5, step=0.1)))\n",
    "    model.add(layers.Dense(4, activation='softmax'))\n",
    "    \n",
    "\n",
    "    # Tune the learning rate for the optimizer\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=hp_learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5,\n",
    "    executions_per_trial=3,\n",
    "    directory='my_dir',\n",
    "    project_name='tuning'\n",
    ")\n",
    "\n",
    "tuner.search(training_dataset,\n",
    "             validation_data=validation_dataset,\n",
    "             epochs=5,\n",
    "             validation_steps=len(validation_dataset))\n",
    "\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "best_hyperparameters = tuner.get_best_hyperparameters(1)[0]\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\")\n",
    "for param, value in best_hyperparameters.values.items():\n",
    "    print(f\"{param}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with dropout rate\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "# Rest of your code for data preprocessing and model definition\n",
    "\n",
    "# Define the function to build the model\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(count_vectorizer)\n",
    "    model.add(layers.Dense(units=hp.Int('units_1', min_value=32, max_value=128, step=32), activation='relu'))\n",
    "    model.add(layers.Dropout(hp.Float('dropout_1', 0, 0.5, step=0.1)))\n",
    "    model.add(layers.Dense(units=hp.Int('units_2', min_value=32, max_value=128, step=32), activation='relu'))\n",
    "    model.add(layers.Dropout(hp.Float('dropout_2', 0, 0.5, step=0.1)))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "        optimizer=tf.keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Define the random search tuner\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5,\n",
    "    executions_per_trial=3,\n",
    "    directory='tuning',\n",
    "    project_name='hyperparameter_tuning'\n",
    ")\n",
    "\n",
    "# Perform the random search\n",
    "tuner.search(training_dataset, validation_data=validation_dataset, epochs=5, steps_per_epoch=len(training_dataset))\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_model = tuner.get_best_models(1)[0]\n",
    "best_hyperparameters = tuner.get_best_hyperparameters(1)[0]\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\")\n",
    "for param, value in best_hyperparameters.values.items():\n",
    "    print(f\"{param}: {value}\")\n",
    "\n",
    "# Evaluate the best model on the test data\n",
    "predictions = best_model.predict(test['content']).round()\n",
    "accuracy = accuracy_score(test['publication'].values, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
